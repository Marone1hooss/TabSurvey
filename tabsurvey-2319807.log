

----------------------------------------------------------------------------
Training STG with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='STG', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stochastic_gates.py", line 34, in fit
    loss, val_loss = self.model.fit(X, y, nr_epochs=self.args.epochs, valid_X=X_val, valid_y=y_val,
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 177, in fit
    return self.train(data_loader, nr_epochs, val_data_loader, verbose, meters, early_stop, print_interval)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 234, in train
    self.train_epoch(data_loader, meters=meters)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 215, in train_epoch
    self.train_step(feed_dict, meters=meters)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 138, in train_step
    loss = as_float(loss)
           ^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/utils.py", line 266, in as_float
    return stmap(_as_float, obj)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/utils.py", line 215, in stmap
    elif isinstance(iterable, (collections.Sequence, collections.UserList)):
                               ^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'collections' has no attribute 'Sequence'


----------------------------------------------------------------------------
Training SAINT with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Namespace(config='config/adult.yml', model_name='SAINT', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Using dim 32 and batch size 128
Using dim 32 and batch size 128
Epoch 0 loss 0.3472060561180115
Epoch 1 loss 0.3215859830379486
Epoch 2 loss 0.31556084752082825
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.317572523570483, 'Log Loss - std': 0.0, 'AUC - mean': 0.9083888048372494, 'AUC - std': 0.0, 'Accuracy - mean': 0.8466144633809305, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8466144633809305, 'F1 score - std': 0.0}
Using dim 32 and batch size 128
Epoch 0 loss 0.3216182589530945
Epoch 1 loss 0.3066885769367218
Epoch 2 loss 0.3026859164237976
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3105150357993961, 'Log Loss - std': 0.007057487771086862, 'AUC - mean': 0.9120852802503399, 'AUC - std': 0.0036964754130905386, 'Accuracy - mean': 0.852514848398082, 'Accuracy - std': 0.005900385017151455, 'F1 score - mean': 0.852514848398082, 'F1 score - std': 0.005900385017151455}
Using dim 32 and batch size 128
Epoch 0 loss 0.3222581446170807
Epoch 1 loss 0.30591368675231934
Epoch 2 loss 0.29807788133621216
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3061500492830043, 'Log Loss - std': 0.00844462177972416, 'AUC - mean': 0.9144728968593178, 'AUC - std': 0.004528875481763995, 'Accuracy - mean': 0.8551982691204248, 'Accuracy - std': 0.006132796190424215, 'F1 score - mean': 0.8551982691204248, 'F1 score - std': 0.006132796190424247}
Using dim 32 and batch size 128
Epoch 0 loss 0.32634222507476807
Epoch 1 loss 0.3093700706958771
Epoch 2 loss 0.30371424555778503
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 46, in cross_validation
    train_energy.end_tracking()
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/utils/energieTracker.py", line 15, in end_tracking
    energy_consumed = self.rapl.result.pkg[0]
                      ~~~~~~~~~~~~~~~~~~~~^^^
TypeError: 'NoneType' object is not subscriptable


----------------------------------------------------------------------------
Training RandomForest with config/adult.yml in env sklearn

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='RandomForest', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
{'Log Loss - mean': 0.3091292211684804, 'Log Loss - std': 0.0, 'AUC - mean': 0.9112425642659868, 'AUC - std': 0.0, 'Accuracy - mean': 0.8513741747274681, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8513741747274681, 'F1 score - std': 0.0}
{'Log Loss - mean': 0.30407837715058106, 'Log Loss - std': 0.0050508440178993474, 'AUC - mean': 0.9151597584762006, 'AUC - std': 0.0039171942102138035, 'Accuracy - mean': 0.8573517065283532, 'Accuracy - std': 0.005977531800885116, 'F1 score - mean': 0.8573517065283532, 'F1 score - std': 0.005977531800885116}
{'Log Loss - mean': 0.29902968611596775, 'Log Loss - std': 0.008245357028134236, 'AUC - mean': 0.918759636705002, 'AUC - std': 0.006012308501018399, 'Accuracy - mean': 0.8598049050893363, 'Accuracy - std': 0.005988067911517588, 'F1 score - mean': 0.8598049050893363, 'F1 score - std': 0.005988067911517618}
{'Log Loss - mean': 0.29983219407489026, 'Log Loss - std': 0.007274715902593984, 'AUC - mean': 0.9180281950176888, 'AUC - std': 0.0053587228851081845, 'Accuracy - mean': 0.8606859883993118, 'Accuracy - std': 0.0054057045222674046, 'F1 score - mean': 0.8606859883993118, 'F1 score - std': 0.005405704522267425}
{'Log Loss - mean': 0.30014894900539996, 'Log Loss - std': 0.006537471064163253, 'AUC - mean': 0.9176070927232522, 'AUC - std': 0.004866419354825984, 'Accuracy - mean': 0.8609689381395969, 'Accuracy - std': 0.004868013488863744, 'F1 score - mean': 0.8609689381395969, 'F1 score - std': 0.00486801348886376}
Results: {'Log Loss - mean': 0.30014894900539996, 'Log Loss - std': 0.006537471064163253, 'AUC - mean': 0.9176070927232522, 'AUC - std': 0.004866419354825984, 'Accuracy - mean': 0.8609689381395969, 'Accuracy - std': 0.004868013488863744, 'F1 score - mean': 0.8609689381395969, 'F1 score - std': 0.00486801348886376}
Train time: 1.5126985398000001
Inference time: 0.05538569580000008
Finished cross validation
{'Log Loss - mean': 0.30014894900539996, 'Log Loss - std': 0.006537471064163253, 'AUC - mean': 0.9176070927232522, 'AUC - std': 0.004866419354825984, 'Accuracy - mean': 0.8609689381395969, 'Accuracy - std': 0.004868013488863744, 'F1 score - mean': 0.8609689381395969, 'F1 score - std': 0.00486801348886376}
(1.5126985398000001, 0.05538569580000008)


----------------------------------------------------------------------------
Training LinearModel with config/adult.yml in env sklearn

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/sklearn/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/marwan.housni/.conda/envs/sklearn/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/marwan.housni/.conda/envs/sklearn/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/marwan.housni/.conda/envs/sklearn/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/marwan.housni/.conda/envs/sklearn/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Namespace(config='config/adult.yml', model_name='LinearModel', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
{'Log Loss - mean': 0.3952096059059315, 'Log Loss - std': 0.0, 'AUC - mean': 0.845634393930956, 'AUC - std': 0.0, 'Accuracy - mean': 0.81759557807462, 'Accuracy - std': 0.0, 'F1 score - mean': 0.81759557807462, 'F1 score - std': 0.0}
{'Log Loss - mean': 0.3885366087979829, 'Log Loss - std': 0.006672997107948581, 'AUC - mean': 0.8503019651445942, 'AUC - std': 0.004667571213638311, 'Accuracy - mean': 0.8207296072191281, 'Accuracy - std': 0.003134029144508166, 'F1 score - mean': 0.8207296072191281, 'F1 score - std': 0.003134029144508166}
{'Log Loss - mean': 0.3851609332737402, 'Log Loss - std': 0.00724405255537739, 'AUC - mean': 0.8531924124839532, 'AUC - std': 0.005588695643839488, 'Accuracy - mean': 0.8230027848291321, 'Accuracy - std': 0.004108864240354539, 'F1 score - mean': 0.8230027848291321, 'F1 score - std': 0.004108864240354539}
{'Log Loss - mean': 0.3844161511444356, 'Log Loss - std': 0.006404789171330822, 'AUC - mean': 0.8536153446520806, 'AUC - std': 0.00489507447401095, 'Accuracy - mean': 0.8234099510297115, 'Accuracy - std': 0.0036275924459524024, 'F1 score - mean': 0.8234099510297115, 'F1 score - std': 0.0036275924459524024}
{'Log Loss - mean': 0.3845945410160526, 'Log Loss - std': 0.005739717000736131, 'AUC - mean': 0.8532316842399261, 'AUC - std': 0.004445017945105384, 'Accuracy - mean': 0.8239306635264718, 'Accuracy - std': 0.0034076542636063485, 'F1 score - mean': 0.8239306635264718, 'F1 score - std': 0.003407654263606335}
Results: {'Log Loss - mean': 0.3845945410160526, 'Log Loss - std': 0.005739717000736131, 'AUC - mean': 0.8532316842399261, 'AUC - std': 0.004445017945105384, 'Accuracy - mean': 0.8239306635264718, 'Accuracy - std': 0.0034076542636063485, 'F1 score - mean': 0.8239306635264718, 'F1 score - std': 0.003407654263606335}
Train time: 0.1234265072
Inference time: 0.0030752496000000296
Finished cross validation
{'Log Loss - mean': 0.3845945410160526, 'Log Loss - std': 0.005739717000736131, 'AUC - mean': 0.8532316842399261, 'AUC - std': 0.004445017945105384, 'Accuracy - mean': 0.8239306635264718, 'Accuracy - std': 0.0034076542636063485, 'F1 score - mean': 0.8239306635264718, 'F1 score - std': 0.003407654263606335}
(0.1234265072, 0.0030752496000000296)


----------------------------------------------------------------------------
Training ModelTree with config/adult.yml in env gbdt

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='ModelTree', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/modeltree.py", line 28, in fit
    X = np.array(X, dtype=np.float)
                          ^^^^^^^^
  File "/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/numpy/__init__.py", line 324, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?


----------------------------------------------------------------------------
Training NAM with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='NAM', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 133, in main_once
    model_name = str2model(args.model_name)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/__init__.py", line 81, in str2model
    from models.neural_additive_models import NAM
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/neural_additive_models.py", line 4, in <module>
    from nam.config import defaults
ModuleNotFoundError: No module named 'nam'


----------------------------------------------------------------------------
Training LightGBM with config/adult.yml in env gbdt

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.
  _log_warning('Overriding the parameters from Reference Dataset.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.
  _log_warning(f'{cat_alias} in param dict is overridden.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.
  _log_warning('Overriding the parameters from Reference Dataset.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.
  _log_warning(f'{cat_alias} in param dict is overridden.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.
  _log_warning('Overriding the parameters from Reference Dataset.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.
  _log_warning(f'{cat_alias} in param dict is overridden.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.
  _log_warning('Overriding the parameters from Reference Dataset.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.
  _log_warning(f'{cat_alias} in param dict is overridden.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.
  _log_warning('Overriding the parameters from Reference Dataset.')
/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.
  _log_warning(f'{cat_alias} in param dict is overridden.')
Namespace(config='config/adult.yml', model_name='LightGBM', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[2]	eval's auc: 0.896125
{'Log Loss - mean': 0.5319043205643598, 'Log Loss - std': 0.0, 'AUC - mean': 0.8961249358010481, 'AUC - std': 0.0, 'Accuracy - mean': 0.7590971902349147, 'Accuracy - std': 0.0, 'F1 score - mean': 0.7590971902349147, 'F1 score - std': 0.0}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.899139
{'Log Loss - mean': 0.5268965814725322, 'Log Loss - std': 0.005007739091827668, 'AUC - mean': 0.8976320541543217, 'AUC - std': 0.001507118353273551, 'Accuracy - mean': 0.759155474724337, 'Accuracy - std': 5.828448942224451e-05, 'F1 score - mean': 0.759155474724337, 'F1 score - std': 5.828448942224451e-05}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.904335
{'Log Loss - mean': 0.5249603723575763, 'Log Loss - std': 0.004920986894403297, 'AUC - mean': 0.8998664531837103, 'AUC - std': 0.003391068936522489, 'Accuracy - mean': 0.7591749028874778, 'Accuracy - std': 5.495114361128626e-05, 'F1 score - mean': 0.7591749028874778, 'F1 score - std': 5.495114361128626e-05}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.898927
{'Log Loss - mean': 0.5241320437416803, 'Log Loss - std': 0.004496717569394689, 'AUC - mean': 0.8996314706317066, 'AUC - std': 0.002964820702576192, 'Accuracy - mean': 0.7591846169690482, 'Accuracy - std': 5.047584848626914e-05, 'F1 score - mean': 0.7591846169690482, 'F1 score - std': 5.047584848626914e-05}
Training until validation scores don't improve for 20 rounds
Did not meet early stopping. Best iteration is:
[3]	eval's auc: 0.892169
{'Log Loss - mean': 0.5238280361918439, 'Log Loss - std': 0.004067684535525391, 'AUC - mean': 0.8981389020890453, 'AUC - std': 0.003992890288613877, 'Accuracy - mean': 0.7591904454179904, 'Accuracy - std': 4.662759153779561e-05, 'F1 score - mean': 0.7591904454179904, 'F1 score - std': 4.662759153779561e-05}
Results: {'Log Loss - mean': 0.5238280361918439, 'Log Loss - std': 0.004067684535525391, 'AUC - mean': 0.8981389020890453, 'AUC - std': 0.003992890288613877, 'Accuracy - mean': 0.7591904454179904, 'Accuracy - std': 4.662759153779561e-05, 'F1 score - mean': 0.7591904454179904, 'F1 score - std': 4.662759153779561e-05}
Train time: 0.06535784379999998
Inference time: 0.0035580139999999093
Finished cross validation
{'Log Loss - mean': 0.5238280361918439, 'Log Loss - std': 0.004067684535525391, 'AUC - mean': 0.8981389020890453, 'AUC - std': 0.003992890288613877, 'Accuracy - mean': 0.7591904454179904, 'Accuracy - std': 4.662759153779561e-05, 'F1 score - mean': 0.7591904454179904, 'F1 score - std': 4.662759153779561e-05}
(0.06535784379999998, 0.0035580139999999093)


----------------------------------------------------------------------------
Training DeepGBM with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/deepgbm_lib/utils/helper.py:52: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/conda/conda-bld/pytorch_1708025569485/work/torch/csrc/utils/python_arg_parser.cpp:1630.)
  p.data.add_(-self.weight_decay, p.data)
Namespace(config='config/adult.yml', model_name='DeepGBM', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
{'task': 'binary', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 3, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.7, 'device': device(type='cpu'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
{'task': 'binary', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 3, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.7, 'device': device(type='cpu'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
Preprocess data for CatNN...
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Info] Number of positive: 6272, number of negative: 19776
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008850 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 643
[LightGBM] [Info] Number of data points in the train set: 26048, number of used features: 33
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240786 -> initscore=-1.148374
[LightGBM] [Info] Start training from score -1.148374
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[81]	valid_0's auc: 0.9199
Model Interpreting...
[(17,), (16,), (16,), (16,), (16,)]

Train embedding model...
Epoch 1: training loss 0.753
Test Loss of 0.340854, Test AUC of 0.901503
Epoch 2: training loss 0.387
Test Loss of 0.301479, Test AUC of 0.914820
Epoch 3: training loss 0.316
Test Loss of 0.297259, Test AUC of 0.917606
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 0.701
Test Loss of 0.694058, Test AUC of 0.896038
Epoch 2: training loss 0.692
Test Loss of 0.692298, Test AUC of 0.899191
Epoch 3: training loss 0.691
Test Loss of 0.692203, Test AUC of 0.900312
Finished Training

################### Make predictions #######################################

{'Log Loss - mean': 0.6922026865825642, 'Log Loss - std': 0.0, 'AUC - mean': 0.900311919244422, 'AUC - std': 0.0, 'Accuracy - mean': 0.2409028097650852, 'Accuracy - std': 0.0, 'F1 score - mean': 0.2409028097650852, 'F1 score - std': 0.0}
{'task': 'binary', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 3, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.7, 'device': device(type='cpu'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
Preprocess data for CatNN...
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Info] Number of positive: 6273, number of negative: 19776
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008465 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 646
[LightGBM] [Info] Number of data points in the train set: 26049, number of used features: 33
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240815 -> initscore=-1.148214
[LightGBM] [Info] Start training from score -1.148214
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[67]	valid_0's auc: 0.927548
Model Interpreting...
[(14,), (14,), (13,), (13,), (13,)]

Train embedding model...
Epoch 1: training loss 0.840
Test Loss of 0.340955, Test AUC of 0.898089
Epoch 2: training loss 0.405
Test Loss of 0.298928, Test AUC of 0.917165
Epoch 3: training loss 0.330
Test Loss of 0.291847, Test AUC of 0.921730
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 0.706
Test Loss of 0.694528, Test AUC of 0.902873
Epoch 2: training loss 0.695
Test Loss of 0.693030, Test AUC of 0.904518
Epoch 3: training loss 0.694
Test Loss of 0.692773, Test AUC of 0.904813
Finished Training

################### Make predictions #######################################

{'Log Loss - mean': 0.692488073563887, 'Log Loss - std': 0.0002853869813227994, 'AUC - mean': 0.9025626466598902, 'AUC - std': 0.002250727415468168, 'Accuracy - mean': 0.240844525275663, 'Accuracy - std': 5.8284489422216756e-05, 'F1 score - mean': 0.240844525275663, 'F1 score - std': 5.8284489422216756e-05}
{'task': 'binary', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 3, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.7, 'device': device(type='cpu'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
Preprocess data for CatNN...
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Info] Number of positive: 6273, number of negative: 19776
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008725 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 644
[LightGBM] [Info] Number of data points in the train set: 26049, number of used features: 33
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240815 -> initscore=-1.148214
[LightGBM] [Info] Start training from score -1.148214
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[89]	valid_0's auc: 0.931802
Model Interpreting...
[(18,), (18,), (18,), (18,), (17,)]

Train embedding model...
Epoch 1: training loss 0.758
Test Loss of 0.326001, Test AUC of 0.907962
Epoch 2: training loss 0.392
Test Loss of 0.287918, Test AUC of 0.922843
Epoch 3: training loss 0.318
Test Loss of 0.281598, Test AUC of 0.926977
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 0.703
Test Loss of 0.692102, Test AUC of 0.907811
Epoch 2: training loss 0.693
Test Loss of 0.689930, Test AUC of 0.910756
Epoch 3: training loss 0.692
Test Loss of 0.689493, Test AUC of 0.911878
Finished Training

################### Make predictions #######################################

{'Log Loss - mean': 0.6914897544192846, 'Log Loss - std': 0.001430936540066718, 'AUC - mean': 0.9056679004627015, 'AUC - std': 0.004760502595200337, 'Accuracy - mean': 0.24082509711252223, 'Accuracy - std': 5.4951143611260094e-05, 'F1 score - mean': 0.24082509711252223, 'F1 score - std': 5.4951143611260094e-05}
{'task': 'binary', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 3, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.7, 'device': device(type='cpu'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
Preprocess data for CatNN...
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Info] Number of positive: 6273, number of negative: 19776
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008511 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 651
[LightGBM] [Info] Number of data points in the train set: 26049, number of used features: 33
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240815 -> initscore=-1.148214
[LightGBM] [Info] Start training from score -1.148214
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[86]	valid_0's auc: 0.924147
Model Interpreting...
[(18,), (17,), (17,), (17,), (17,)]

Train embedding model...
Epoch 1: training loss 0.744
Test Loss of 0.323302, Test AUC of 0.905186
Epoch 2: training loss 0.383
Test Loss of 0.295168, Test AUC of 0.918157
Epoch 3: training loss 0.319
Test Loss of 0.291413, Test AUC of 0.921069
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 0.704
Test Loss of 0.691673, Test AUC of 0.902346
Epoch 2: training loss 0.693
Test Loss of 0.690029, Test AUC of 0.905038
Epoch 3: training loss 0.692
Test Loss of 0.689687, Test AUC of 0.905902
Finished Training

################### Make predictions #######################################

{'Log Loss - mean': 0.6910391783522329, 'Log Loss - std': 0.001464493398029838, 'AUC - mean': 0.9057265144449226, 'AUC - std': 0.004123965993545205, 'Accuracy - mean': 0.24081538303095187, 'Accuracy - std': 5.047584848624511e-05, 'F1 score - mean': 0.24081538303095187, 'F1 score - std': 5.047584848624511e-05}
{'task': 'binary', 'batch_size': 128, 'test_batch_size': 256, 'l2_reg': 1e-06, 'l2_reg_opt': 0.0005, 'lr': 0.001, 'epochs': 3, 'early_stopping': 3, 'loss_de': 10, 'loss_dr': 0.7, 'device': device(type='cpu'), 'bins': 32, 'rate': 0.9, 'threshold': 10, 'maxleaf': 64, 'num_slices': 5, 'n_feature': 128, 'n_clusters': 10, 'tree_lr': 0.1, 'n_trees': 200, 'embsize': 20, 'emb_lr': 0.001, 'emb_epochs': 3, 'tree_layers': [100, 100, 100, 50, 20], 'embedding_size': 20, 'cate_layers': [16, 16], 'online_epochs': 1, 'online_bz': 4096, 'num_splits': 5, 'early-stopping': 20}
Preprocess data for CatNN...
Preprocess data for GBDT2NN...

################### Train model #######################################


Train GBDT and distill knowledge from it...
[LightGBM] [Info] Number of positive: 6273, number of negative: 19776
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008285 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 649
[LightGBM] [Info] Number of data points in the train set: 26049, number of used features: 33
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240815 -> initscore=-1.148214
[LightGBM] [Info] Start training from score -1.148214
Training until validation scores don't improve for 20 rounds
Early stopping, best iteration is:
[67]	valid_0's auc: 0.923909
Model Interpreting...
[(14,), (14,), (13,), (13,), (13,)]

Train embedding model...
Epoch 1: training loss 0.780
Test Loss of 0.344273, Test AUC of 0.896537
Epoch 2: training loss 0.396
Test Loss of 0.302817, Test AUC of 0.914097
Epoch 3: training loss 0.323
Test Loss of 0.296472, Test AUC of 0.918621
Finished Training

Train DeepGBM model...
Init GBDT2NN succeed!
Init fm part
Init fm part succeed
Init deep part
Init deep part succeed
Init CatNN succeed!
Init DeepGBM succeed!
Epoch 1: training loss 0.707
Test Loss of 0.695358, Test AUC of 0.899479
Epoch 2: training loss 0.695
Test Loss of 0.693666, Test AUC of 0.902613
Epoch 3: training loss 0.694
Test Loss of 0.693324, Test AUC of 0.903720
Finished Training

################### Make predictions #######################################

{'Log Loss - mean': 0.6914962176484281, 'Log Loss - std': 0.0015972890794607455, 'AUC - mean': 0.9053253030335485, 'AUC - std': 0.0037748587787095962, 'Accuracy - mean': 0.24080955458200964, 'Accuracy - std': 4.662759153777341e-05, 'F1 score - mean': 0.24080955458200964, 'F1 score - std': 4.662759153777341e-05}
Results: {'Log Loss - mean': 0.6914962176484281, 'Log Loss - std': 0.0015972890794607455, 'AUC - mean': 0.9053253030335485, 'AUC - std': 0.0037748587787095962, 'Accuracy - mean': 0.24080955458200964, 'Accuracy - std': 4.662759153777341e-05, 'F1 score - mean': 0.24080955458200964, 'F1 score - std': 4.662759153777341e-05}
Train time: 36.253258517599996
Inference time: 0.44984396800000753
Finished cross validation
{'Log Loss - mean': 0.6914962176484281, 'Log Loss - std': 0.0015972890794607455, 'AUC - mean': 0.9053253030335485, 'AUC - std': 0.0037748587787095962, 'Accuracy - mean': 0.24080955458200964, 'Accuracy - std': 4.662759153777341e-05, 'F1 score - mean': 0.24080955458200964, 'F1 score - std': 4.662759153777341e-05}
(36.253258517599996, 0.44984396800000753)


----------------------------------------------------------------------------
Training TabNet with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu
  warnings.warn(f"Device used : {self.device}")
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu
  warnings.warn(f"Device used : {self.device}")
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)
Namespace(config='config/adult.yml', model_name='TabNet', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
epoch 0  | loss: 0.39562 | eval_logloss: 0.35918 |  0:00:02s
epoch 1  | loss: 0.34924 | eval_logloss: 0.33656 |  0:00:05s
epoch 2  | loss: 0.33367 | eval_logloss: 0.33911 |  0:00:07s
Stop training because you reached max_epochs = 3 with best_epoch = 1 and best_eval_logloss = 0.33656
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 50, in cross_validation
    curr_model.predict(X_test)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/basemodel_torch.py", line 123, in predict
    self.predict_proba(X)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/basemodel_torch.py", line 129, in predict_proba
    probas = self.predict_helper(X)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/tabnet.py", line 48, in predict_helper
    X = np.array(X, dtype=np.float)
                          ^^^^^^^^
  File "/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/numpy/__init__.py", line 324, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?


----------------------------------------------------------------------------
Training DeepFM with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='DeepFM', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/deepfm.py", line 44, in fit
    X = np.array(X, dtype=np.float)
                          ^^^^^^^^
  File "/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/numpy/__init__.py", line 324, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?


----------------------------------------------------------------------------
Training KNN with config/adult.yml in env sklearn

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='KNN', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
{'Log Loss - mean': 0.4837176129593064, 'Log Loss - std': 0.0, 'AUC - mean': 0.8585548841737467, 'AUC - std': 0.0, 'Accuracy - mean': 0.8148318747121143, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8148318747121143, 'F1 score - std': 0.0}
{'Log Loss - mean': 0.47258170794539245, 'Log Loss - std': 0.011135905013913955, 'AUC - mean': 0.8627011111600852, 'AUC - std': 0.004146226986338486, 'Accuracy - mean': 0.8207298194199393, 'Accuracy - std': 0.0058979447078249545, 'F1 score - mean': 0.8207298194199392, 'F1 score - std': 0.005897944707824898}
{'Log Loss - mean': 0.46723611198307213, 'Log Loss - std': 0.011824679532120946, 'AUC - mean': 0.8656560752817889, 'AUC - std': 0.005378143242415587, 'Accuracy - mean': 0.8245385528319661, 'Accuracy - std': 0.007225192149150958, 'F1 score - mean': 0.8245385528319661, 'F1 score - std': 0.007225192149150986}
{'Log Loss - mean': 0.46675393050251796, 'Log Loss - std': 0.010274472320688212, 'AUC - mean': 0.865083637139426, 'AUC - std': 0.004761971737888724, 'Accuracy - mean': 0.823525229120289, 'Accuracy - std': 0.006498694178683528, 'F1 score - mean': 0.823525229120289, 'F1 score - std': 0.006498694178683552}
{'Log Loss - mean': 0.4660345402115609, 'Log Loss - std': 0.00930171568785908, 'AUC - mean': 0.8655202890912053, 'AUC - std': 0.004347845394437495, 'Accuracy - mean': 0.8228558098318578, 'Accuracy - std': 0.005964805911120618, 'F1 score - mean': 0.8228558098318578, 'F1 score - std': 0.005964805911120639}
Results: {'Log Loss - mean': 0.4660345402115609, 'Log Loss - std': 0.00930171568785908, 'AUC - mean': 0.8655202890912053, 'AUC - std': 0.004347845394437495, 'Accuracy - mean': 0.8228558098318578, 'Accuracy - std': 0.005964805911120618, 'F1 score - mean': 0.8228558098318578, 'F1 score - std': 0.005964805911120639}
Train time: 0.02200843880000001
Inference time: 0.5258947478
Finished cross validation
{'Log Loss - mean': 0.4660345402115609, 'Log Loss - std': 0.00930171568785908, 'AUC - mean': 0.8655202890912053, 'AUC - std': 0.004347845394437495, 'Accuracy - mean': 0.8228558098318578, 'Accuracy - std': 0.005964805911120618, 'F1 score - mean': 0.8228558098318578, 'F1 score - std': 0.005964805911120639}
(0.02200843880000001, 0.5258947478)


----------------------------------------------------------------------------
Training NODE with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/node_lib/odst.py:17: SyntaxWarning: invalid escape sequence '\i'
  """
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/qhoptim/pyt/qhadam.py:133: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at /opt/conda/conda-bld/pytorch_1708025569485/work/torch/csrc/utils/python_arg_parser.cpp:1630.)
  exp_avg.mul_(beta1_adj).add_(1.0 - beta1_adj, d_p)
Namespace(config='config/adult.yml', model_name='NODE', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
On: cpu
On Device: cpu
On: cpu
On Device: cpu
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_temp_100.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_avg.pth
Loss 0.50420
Val Loss: 0.48621
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_temp_100.pth
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_temp_200.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_avg.pth
Loss 0.39889
Val Loss: 0.47414
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_temp_200.pth
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_temp_300.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_avg.pth
Loss 0.43446
Val Loss: 0.46378
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_temp_300.pth
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_temp_400.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_avg.pth
Loss 0.42252
Val Loss: 0.45543
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_temp_400.pth
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_temp_500.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_avg.pth
Loss 0.37658
Val Loss: 0.44907
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_temp_500.pth
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_temp_600.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_avg.pth
Loss 0.35582
Val Loss: 0.43367
Saved logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_temp_600.pth
Loaded logs/Adult_2024.03.18_23:11:47/checkpoint_best.pth
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 50, in cross_validation
    curr_model.predict(X_test)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/basemodel_torch.py", line 123, in predict
    self.predict_proba(X)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/basemodel_torch.py", line 129, in predict_proba
    probas = self.predict_helper(X)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/node.py", line 142, in predict_helper
    X_test = torch.as_tensor(np.array(X, dtype=np.float), device=self.device, dtype=torch.float32)
                                               ^^^^^^^^
  File "/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/numpy/__init__.py", line 324, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?


----------------------------------------------------------------------------
Training DANet with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='DANet', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 133, in main_once
    model_name = str2model(args.model_name)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/__init__.py", line 93, in str2model
    from models.danet import DANet
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/danet.py", line 5, in <module>
    from models.danet_lib.DAN_Task import DANetClassifier, DANetRegressor
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/danet_lib/DAN_Task.py", line 5, in <module>
    from models.danet_lib.abstract_model import DANsModel
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/danet_lib/abstract_model.py", line 23, in <module>
    from models.danet_lib.lib.logger import Train_Log
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/danet_lib/lib/logger.py", line 4, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/tensorboard/__init__.py", line 1, in <module>
    import tensorboard
ModuleNotFoundError: No module named 'tensorboard'


----------------------------------------------------------------------------
Training XGBoost with config/adult.yml in env gbdt

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='XGBoost', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
[0]	eval-auc:0.86786
[2]	eval-auc:0.88346
{'Log Loss - mean': 0.6124011791520276, 'Log Loss - std': 0.0, 'AUC - mean': 0.8834578896128674, 'AUC - std': 0.0, 'Accuracy - mean': 0.844311377245509, 'Accuracy - std': 0.0, 'F1 score - mean': 0.844311377245509, 'F1 score - std': 0.0}
[0]	eval-auc:0.88021
[2]	eval-auc:0.88652
{'Log Loss - mean': 0.6122289228619955, 'Log Loss - std': 0.00017225629003220222, 'AUC - mean': 0.884990379765733, 'AUC - std': 0.001532490152865551, 'Accuracy - mean': 0.8494437721608381, 'Accuracy - std': 0.005132394915329075, 'F1 score - mean': 0.8494437721608381, 'F1 score - std': 0.005132394915329075}
[0]	eval-auc:0.88692
[2]	eval-auc:0.90235
{'Log Loss - mean': 0.6115491955028886, 'Log Loss - std': 0.0009715143085620946, 'AUC - mean': 0.8907756964823091, 'AUC - std': 0.008276802708047582, 'Accuracy - mean': 0.8519735712849487, 'Accuracy - std': 0.005510059188547583, 'F1 score - mean': 0.8519735712849487, 'F1 score - std': 0.005510059188547583}
[0]	eval-auc:0.87333
[2]	eval-auc:0.88746
{'Log Loss - mean': 0.6112649727553002, 'Log Loss - std': 0.0009747962605311391, 'AUC - mean': 0.8899463753024227, 'AUC - std': 0.007310432145857212, 'Accuracy - mean': 0.8528545642130972, 'Accuracy - std': 0.005009891197852188, 'F1 score - mean': 0.8528545642130972, 'F1 score - std': 0.005009891197852188}
[0]	eval-auc:0.87765
[2]	eval-auc:0.88765
{'Log Loss - mean': 0.611338277112932, 'Log Loss - std': 0.0008841246037904609, 'AUC - mean': 0.8894874555272562, 'AUC - std': 0.006602754271325076, 'Accuracy - mean': 0.8527689093557358, 'Accuracy - std': 0.004484256334088397, 'F1 score - mean': 0.8527689093557358, 'F1 score - std': 0.004484256334088397}
Results: {'Log Loss - mean': 0.611338277112932, 'Log Loss - std': 0.0008841246037904609, 'AUC - mean': 0.8894874555272562, 'AUC - std': 0.006602754271325076, 'Accuracy - mean': 0.8527689093557358, 'Accuracy - std': 0.004484256334088397, 'F1 score - mean': 0.8527689093557358, 'F1 score - std': 0.004484256334088397}
Train time: 0.12292665100000004
Inference time: 0.0045904928000000565
Finished cross validation
{'Log Loss - mean': 0.611338277112932, 'Log Loss - std': 0.0008841246037904609, 'AUC - mean': 0.8894874555272562, 'AUC - std': 0.006602754271325076, 'Accuracy - mean': 0.8527689093557358, 'Accuracy - std': 0.004484256334088397, 'F1 score - mean': 0.8527689093557358, 'F1 score - std': 0.004484256334088397}
(0.12292665100000004, 0.0045904928000000565)


----------------------------------------------------------------------------
Training TabTransformer with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='TabTransformer', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
[9, 16, 7, 15, 6, 5, 2, 42]
On Device: cpu
Using dim 128 and batch size 128
On Device: cpu
[9, 16, 7, 15, 6, 5, 2, 42]
On Device: cpu
Using dim 128 and batch size 128
On Device: cpu
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/tabtransformer.py", line 59, in fit
    X = np.array(X, dtype=np.float)
                          ^^^^^^^^
  File "/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/numpy/__init__.py", line 324, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?


----------------------------------------------------------------------------
Training RLN with config/adult.yml in env tensorflow

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Using TensorFlow backend.
WARNING:tensorflow:From /home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2024-03-19 00:09:33.663958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-03-19 00:09:33.837839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA A100-SXM4-80GB major: 8 minor: 0 memoryClockRate(GHz): 1.41
pciBusID: 0000:81:00.0
2024-03-19 00:09:33.867983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-19 00:09:34.361899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-19 00:09:34.591100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-03-19 00:09:34.695813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-03-19 00:09:35.064217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-03-19 00:09:35.340314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-03-19 00:09:36.043086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-03-19 00:09:36.048214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-03-19 00:09:36.048906: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2024-03-19 00:09:36.090117: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1996355000 Hz
2024-03-19 00:09:36.090357: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1fd8910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-03-19 00:09:36.090724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-03-19 00:09:36.310805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3f245f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-19 00:09:36.310871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0
2024-03-19 00:09:36.312213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA A100-SXM4-80GB major: 8 minor: 0 memoryClockRate(GHz): 1.41
pciBusID: 0000:81:00.0
2024-03-19 00:09:36.312265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-19 00:09:36.312297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-19 00:09:36.312324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-03-19 00:09:36.312350: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-03-19 00:09:36.312375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-03-19 00:09:36.312400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-03-19 00:09:36.312463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-03-19 00:09:36.314910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-03-19 00:09:36.335694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-19 00:09:36.337593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-03-19 00:09:36.337623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-03-19 00:09:36.337642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-03-19 00:09:36.340221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 76595 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:81:00.0, compute capability: 8.0)
WARNING:tensorflow:From /home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

2024-03-19 00:14:50.385993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-19 00:16:17.675436: E tensorflow/stream_executor/cuda/cuda_blas.cc:428] failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
Namespace(batch_size=128, cat_dims=[9, 16, 7, 15, 6, 5, 2, 42], cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], config='config/adult.yml', data_parallel=True, dataset='Adult', direction='maximize', early_stopping_rounds=20, epochs=3, gpu_ids=[0, 1], logging_period=100, model_name='RLN', n_trials=2, num_classes=1, num_features=14, num_splits=5, objective='binary', one_hot_encode=False, optimize_hyperparameters=False, scale=True, seed=221, shuffle=True, target_encode=True, use_gpu=False, val_batch_size=256)
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Train on 26048 samples, validate on 6513 samples
Epoch 1/3
Traceback (most recent call last):
  File "train.py", line 154, in <module>
    main_once(arguments)
  File "train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
  File "train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/rln.py", line 53, in fit
    history = self.model.fit(X, y, validation_data=(X_val, y_val))
  File "/home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py", line 209, in fit
    return super(KerasClassifier, self).fit(x, y, **kwargs)
  File "/home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py", line 151, in fit
    history = self.model.fit(x, y, **fit_args)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/rln.py", line 107, in rln_fit
    return orig_fit(*args, callbacks=rln_callbacks, **fit_kwargs)
  File "/home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py", line 1239, in fit
    validation_freq=validation_freq)
  File "/home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training_arrays.py", line 196, in fit_loop
    outs = fit_function(ins_batch)
  File "/home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Blas GEMM launch failed : a.shape=(128, 14), b.shape=(14, 8), m=128, n=8, k=14
	 [[{{node dense_1/MatMul}}]]
  (1) Internal: Blas GEMM launch failed : a.shape=(128, 14), b.shape=(14, 8), m=128, n=8, k=14
	 [[{{node dense_1/MatMul}}]]
	 [[Mean/_109]]
0 successful operations.
0 derived errors ignored.


----------------------------------------------------------------------------
Training DNFNet with config/adult.yml in env tensorflow

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf.py:54: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf.py:55: The name tf.random.set_random_seed is deprecated. Please use tf.compat.v1.random.set_random_seed instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/ModelHandler.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/DNFNetComponents.py:52: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/DNFNetComponents.py:57: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/DNFNetComponents.py:60: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/DNFNetComponents.py:162: The name tf.sparse.matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/model1.py:56: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/model1.py:58: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/model1.py:63: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.

WARNING:tensorflow:From /home/marwan.housni/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/model1.py:67: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/DNFNetModels/model1.py:71: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/ModelHandler.py:150: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2024-03-19 00:16:26.015741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-03-19 00:16:26.076597: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2024-03-19 00:16:26.076659: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: slurm-a100-gpu-h22a2-u14-sv
2024-03-19 00:16:26.076682: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: slurm-a100-gpu-h22a2-u14-sv
2024-03-19 00:16:26.076781: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 535.154.5
2024-03-19 00:16:26.076819: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 535.154.5
2024-03-19 00:16:26.076839: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 535.154.5
2024-03-19 00:16:26.077013: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2024-03-19 00:16:26.093107: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1996355000 Hz
2024-03-19 00:16:26.093290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x701dbb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-03-19 00:16:26.093338: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/ModelHandler.py:168: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/ModelHandler.py:169: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/dnf_lib/DNFNet/ModelHandler.py:169: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Namespace(batch_size=128, cat_dims=[9, 16, 7, 15, 6, 5, 2, 42], cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], config='config/adult.yml', data_parallel=True, dataset='Adult', direction='maximize', early_stopping_rounds=20, epochs=3, gpu_ids=[0, 1], logging_period=100, model_name='DNFNet', n_trials=2, num_classes=1, num_features=14, num_splits=5, objective='binary', one_hot_encode=False, optimize_hyperparameters=False, scale=True, seed=221, shuffle=True, target_encode=True, use_gpu=False, val_batch_size=256)
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
{'n_conjunctions_arr': [6, 9, 12, 15], 'conjunctions_depth_arr': [2, 4, 6], 'keep_feature_prob_arr': [0.1, 0.3, 0.5, 0.7, 0.9], 'initial_lr': 0.05, 'lr_decay_factor': 0.5, 'lr_patience': 10, 'min_lr': 1e-06, 'early_stopping_patience': 20, 'epochs': 3, 'batch_size': 128, 'apply_standardization': True, 'save_weights': True, 'starting_epoch_to_save': 0, 'models_module_name': 'models.dnf_lib.DNFNet.DNFNetModels', 'models_dir': 'models/dnf_lib/DNFNet/DNFNetModels', 'model_number': 1, 'experiments_dir': 'output/DNFNet/experiments/Adult', 'competition_name': 'Adult', 'model_name': 'DNFNet', 'n_formulas': 64, 'orthogonal_lambda': 0.0, 'elastic_net_beta': 1.3, 'random_seed': 1306, 'input_dim': 14, 'output_dim': 1, 'translate_label_to_one_hot': False, 'experiment_number': 1, 'GPU': '[0, 1]', 'n_forumlas': 1024}
{'n_conjunctions_arr': [6, 9, 12, 15], 'conjunctions_depth_arr': [2, 4, 6], 'keep_feature_prob_arr': [0.1, 0.3, 0.5, 0.7, 0.9], 'initial_lr': 0.05, 'lr_decay_factor': 0.5, 'lr_patience': 10, 'min_lr': 1e-06, 'early_stopping_patience': 20, 'epochs': 3, 'batch_size': 128, 'apply_standardization': True, 'save_weights': True, 'starting_epoch_to_save': 0, 'models_module_name': 'models.dnf_lib.DNFNet.DNFNetModels', 'models_dir': 'models/dnf_lib/DNFNet/DNFNetModels', 'model_number': 1, 'experiments_dir': 'output/DNFNet/experiments/Adult', 'competition_name': 'Adult', 'model_name': 'DNFNet', 'n_formulas': 64, 'orthogonal_lambda': 0.0, 'elastic_net_beta': 1.3, 'random_seed': 1306, 'input_dim': 14, 'output_dim': 1, 'translate_label_to_one_hot': False, 'experiment_number': 1, 'GPU': '[0, 1]', 'n_forumlas': 1024}
Build optimizer
Epoch: 0, loss: 0.493480, val loss: 0.401061, score: 0.387253
Val score improved from inf to 0.38725299835513943
saving model weights.
Epoch: 1, loss: 0.366083, val loss: 0.362207, score: 0.355958
Val score improved from 0.38725299835513943 to 0.35595806834439375
saving model weights.
Epoch: 2, loss: 0.341517, val loss: 0.345402, score: 0.338620
Val score improved from 0.35595806834439375 to 0.33861964912011216
saving model weights.
Best validation score: 0.33861964912011216
{'Log Loss - mean': 0.3386196490243059, 'Log Loss - std': 0.0, 'AUC - mean': 0.8972916034990234, 'AUC - std': 0.0, 'Accuracy - mean': 0.8350990327038231, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8350990327038232, 'F1 score - std': 0.0}
{'n_conjunctions_arr': [6, 9, 12, 15], 'conjunctions_depth_arr': [2, 4, 6], 'keep_feature_prob_arr': [0.1, 0.3, 0.5, 0.7, 0.9], 'initial_lr': 0.05, 'lr_decay_factor': 0.5, 'lr_patience': 10, 'min_lr': 1e-06, 'early_stopping_patience': 20, 'epochs': 3, 'batch_size': 128, 'apply_standardization': True, 'save_weights': True, 'starting_epoch_to_save': 0, 'models_module_name': 'models.dnf_lib.DNFNet.DNFNetModels', 'models_dir': 'models/dnf_lib/DNFNet/DNFNetModels', 'model_number': 1, 'experiments_dir': 'output/DNFNet/experiments/Adult', 'competition_name': 'Adult', 'model_name': 'DNFNet', 'n_formulas': 64, 'orthogonal_lambda': 0.0, 'elastic_net_beta': 1.3, 'random_seed': 1306, 'input_dim': 14, 'output_dim': 1, 'translate_label_to_one_hot': False, 'experiment_number': 1, 'GPU': '[0, 1]', 'n_forumlas': 1024}
Build optimizer
Epoch: 0, loss: 0.490832, val loss: 0.390771, score: 0.381672
Val score improved from inf to 0.38167154256378
saving model weights.
Epoch: 1, loss: 0.364324, val loss: 0.349572, score: 0.340490
Val score improved from 0.38167154256378 to 0.34048966970895383
saving model weights.
Epoch: 2, loss: 0.345700, val loss: 0.337085, score: 0.331856
Val score improved from 0.34048966970895383 to 0.3318556922997625
saving model weights.
Best validation score: 0.3318556922997625
{'Log Loss - mean': 0.33523767069542854, 'Log Loss - std': 0.0033819783288773775, 'AUC - mean': 0.8990491199335815, 'AUC - std': 0.0017575164345580707, 'Accuracy - mean': 0.8406914082438035, 'Accuracy - std': 0.005592375539980321, 'F1 score - mean': 0.8406914082438035, 'F1 score - std': 0.0055923755399802655}
{'n_conjunctions_arr': [6, 9, 12, 15], 'conjunctions_depth_arr': [2, 4, 6], 'keep_feature_prob_arr': [0.1, 0.3, 0.5, 0.7, 0.9], 'initial_lr': 0.05, 'lr_decay_factor': 0.5, 'lr_patience': 10, 'min_lr': 1e-06, 'early_stopping_patience': 20, 'epochs': 3, 'batch_size': 128, 'apply_standardization': True, 'save_weights': True, 'starting_epoch_to_save': 0, 'models_module_name': 'models.dnf_lib.DNFNet.DNFNetModels', 'models_dir': 'models/dnf_lib/DNFNet/DNFNetModels', 'model_number': 1, 'experiments_dir': 'output/DNFNet/experiments/Adult', 'competition_name': 'Adult', 'model_name': 'DNFNet', 'n_formulas': 64, 'orthogonal_lambda': 0.0, 'elastic_net_beta': 1.3, 'random_seed': 1306, 'input_dim': 14, 'output_dim': 1, 'translate_label_to_one_hot': False, 'experiment_number': 1, 'GPU': '[0, 1]', 'n_forumlas': 1024}
Build optimizer
Epoch: 0, loss: 0.498529, val loss: 0.403240, score: 0.386075
Val score improved from inf to 0.3860753781301786
saving model weights.
Epoch: 1, loss: 0.374530, val loss: 0.348588, score: 0.340825
Val score improved from 0.3860753781301786 to 0.3408247481960648
saving model weights.
Epoch: 2, loss: 0.350435, val loss: 0.332737, score: 0.329174
Val score improved from 0.3408247481960648 to 0.32917361420762886
saving model weights.
Best validation score: 0.32917361420762886
{'Log Loss - mean': 0.33321631851290173, 'Log Loss - std': 0.003974533209859516, 'AUC - mean': 0.9011185391311387, 'AUC - std': 0.003259483748836812, 'Accuracy - mean': 0.8437840346522982, 'Accuracy - std': 0.006322851597590342, 'F1 score - mean': 0.8437840346522982, 'F1 score - std': 0.006322851597590328}
{'n_conjunctions_arr': [6, 9, 12, 15], 'conjunctions_depth_arr': [2, 4, 6], 'keep_feature_prob_arr': [0.1, 0.3, 0.5, 0.7, 0.9], 'initial_lr': 0.05, 'lr_decay_factor': 0.5, 'lr_patience': 10, 'min_lr': 1e-06, 'early_stopping_patience': 20, 'epochs': 3, 'batch_size': 128, 'apply_standardization': True, 'save_weights': True, 'starting_epoch_to_save': 0, 'models_module_name': 'models.dnf_lib.DNFNet.DNFNetModels', 'models_dir': 'models/dnf_lib/DNFNet/DNFNetModels', 'model_number': 1, 'experiments_dir': 'output/DNFNet/experiments/Adult', 'competition_name': 'Adult', 'model_name': 'DNFNet', 'n_formulas': 64, 'orthogonal_lambda': 0.0, 'elastic_net_beta': 1.3, 'random_seed': 1306, 'input_dim': 14, 'output_dim': 1, 'translate_label_to_one_hot': False, 'experiment_number': 1, 'GPU': '[0, 1]', 'n_forumlas': 1024}
Build optimizer
Epoch: 0, loss: 0.492808, val loss: 0.391757, score: 0.376710
Val score improved from inf to 0.37670973401988056
saving model weights.
Epoch: 1, loss: 0.369431, val loss: 0.353849, score: 0.346589
Val score improved from 0.37670973401988056 to 0.346589366117085
saving model weights.
Epoch: 2, loss: 0.348787, val loss: 0.343601, score: 0.337246
Val score improved from 0.346589366117085 to 0.33724586605906703
saving model weights.
Best validation score: 0.33724586605906703
{'Log Loss - mean': 0.3342237054301916, 'Log Loss - std': 0.0038590375588464366, 'AUC - mean': 0.9004669192375621, 'AUC - std': 0.003040065984285989, 'Accuracy - mean': 0.8446009252521229, 'Accuracy - std': 0.005655596343455665, 'F1 score - mean': 0.8446009252521229, 'F1 score - std': 0.005655596343455644}
{'n_conjunctions_arr': [6, 9, 12, 15], 'conjunctions_depth_arr': [2, 4, 6], 'keep_feature_prob_arr': [0.1, 0.3, 0.5, 0.7, 0.9], 'initial_lr': 0.05, 'lr_decay_factor': 0.5, 'lr_patience': 10, 'min_lr': 1e-06, 'early_stopping_patience': 20, 'epochs': 3, 'batch_size': 128, 'apply_standardization': True, 'save_weights': True, 'starting_epoch_to_save': 0, 'models_module_name': 'models.dnf_lib.DNFNet.DNFNetModels', 'models_dir': 'models/dnf_lib/DNFNet/DNFNetModels', 'model_number': 1, 'experiments_dir': 'output/DNFNet/experiments/Adult', 'competition_name': 'Adult', 'model_name': 'DNFNet', 'n_formulas': 64, 'orthogonal_lambda': 0.0, 'elastic_net_beta': 1.3, 'random_seed': 1306, 'input_dim': 14, 'output_dim': 1, 'translate_label_to_one_hot': False, 'experiment_number': 1, 'GPU': '[0, 1]', 'n_forumlas': 1024}
Build optimizer
Epoch: 0, loss: 0.493592, val loss: 0.386024, score: 0.376298
Val score improved from inf to 0.3762977831855105
saving model weights.
Epoch: 1, loss: 0.363335, val loss: 0.354276, score: 0.346215
Val score improved from 0.3762977831855105 to 0.3462154603973992
saving model weights.
Epoch: 2, loss: 0.343386, val loss: 0.345621, score: 0.339594
Val score improved from 0.3462154603973992 to 0.33959411422742636
saving model weights.
Best validation score: 0.33959411422742636
{'Log Loss - mean': 0.3352977872042548, 'Log Loss - std': 0.004065506528585248, 'AUC - mean': 0.8996612903888938, 'AUC - std': 0.0031606569440981707, 'Accuracy - mean': 0.8454903225112806, 'Accuracy - std': 0.00536215683643033, 'F1 score - mean': 0.8454903225112806, 'F1 score - std': 0.005362156836430305}
Results: {'Log Loss - mean': 0.3352977872042548, 'Log Loss - std': 0.004065506528585248, 'AUC - mean': 0.8996612903888938, 'AUC - std': 0.0031606569440981707, 'Accuracy - mean': 0.8454903225112806, 'Accuracy - std': 0.00536215683643033, 'F1 score - mean': 0.8454903225112806, 'F1 score - std': 0.005362156836430305}
Train time: 3.2506684794000003
Inference time: 0.18956163319999994
Finished cross validation
{'Log Loss - mean': 0.3352977872042548, 'Log Loss - std': 0.004065506528585248, 'AUC - mean': 0.8996612903888938, 'AUC - std': 0.0031606569440981707, 'Accuracy - mean': 0.8454903225112806, 'Accuracy - std': 0.00536215683643033, 'F1 score - mean': 0.8454903225112806, 'F1 score - std': 0.005362156836430305}
(3.2506684794000003, 0.18956163319999994)


----------------------------------------------------------------------------
Training VIME with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Namespace(config='config/adult.yml', model_name='VIME', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
On Device: cpu
On Device: cpu
Fitted encoder
Epoch 0, Val Loss: 0.39996
Epoch 1, Val Loss: 0.39671
Epoch 2, Val Loss: 0.37253
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3732269082841558, 'Log Loss - std': 0.0, 'AUC - mean': 0.884684373201656, 'AUC - std': 0.0, 'Accuracy - mean': 0.8275756179947796, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8275756179947795, 'F1 score - std': 0.0}
On Device: cpu
Fitted encoder
Epoch 0, Val Loss: 0.43896
Epoch 1, Val Loss: 0.43316
Epoch 2, Val Loss: 0.43386
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.4034085220678736, 'Log Loss - std': 0.03018161378371781, 'AUC - mean': 0.8613551251348582, 'AUC - std': 0.023329248066797814, 'Accuracy - mean': 0.8175040252136061, 'Accuracy - std': 0.010071592781173588, 'F1 score - mean': 0.817504025213606, 'F1 score - std': 0.010071592781173533}
On Device: cpu
Fitted encoder
Epoch 0, Val Loss: 0.39841
Epoch 1, Val Loss: 0.39213
Epoch 2, Val Loss: 0.39100
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3991265785962126, 'Log Loss - std': 0.025376300364355776, 'AUC - mean': 0.8684838911995856, 'AUC - std': 0.021551670193266755, 'Accuracy - mean': 0.816143142116196, 'Accuracy - std': 0.008445629621851258, 'F1 score - mean': 0.816143142116196, 'F1 score - std': 0.008445629621851207}
On Device: cpu
Fitted encoder
Epoch 0, Val Loss: 0.41678
Epoch 1, Val Loss: 0.36858
Epoch 2, Val Loss: 0.37282
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.39151468555178026, 'Log Loss - std': 0.02562791861029948, 'AUC - mean': 0.8742069378718591, 'AUC - std': 0.021133283127062162, 'Accuracy - mean': 0.8220658946706851, 'Accuracy - std': 0.01259894790239407, 'F1 score - mean': 0.8220658946706851, 'F1 score - std': 0.012598947902394056}
On Device: cpu
Fitted encoder
Epoch 0, Val Loss: 0.41085
Epoch 1, Val Loss: 0.40690
Epoch 2, Val Loss: 0.39258
{'Log Loss - mean': 0.391701576333442, 'Log Loss - std': 0.022925354577279455, 'AUC - mean': 0.8773734221355429, 'AUC - std': 0.019934869323662112, 'Accuracy - mean': 0.8259573840412164, 'Accuracy - std': 0.013695311193310794, 'F1 score - mean': 0.8259573840412164, 'F1 score - std': 0.013695311193310791}
Results: {'Log Loss - mean': 0.391701576333442, 'Log Loss - std': 0.022925354577279455, 'AUC - mean': 0.8773734221355429, 'AUC - std': 0.019934869323662112, 'Accuracy - mean': 0.8259573840412164, 'Accuracy - std': 0.013695311193310794, 'F1 score - mean': 0.8259573840412164, 'F1 score - std': 0.013695311193310791}
Train time: 18.3656079956
Inference time: 0.040667515400000555
Finished cross validation
{'Log Loss - mean': 0.391701576333442, 'Log Loss - std': 0.022925354577279455, 'AUC - mean': 0.8773734221355429, 'AUC - std': 0.019934869323662112, 'Accuracy - mean': 0.8259573840412164, 'Accuracy - std': 0.013695311193310794, 'F1 score - mean': 0.8259573840412164, 'F1 score - std': 0.013695311193310791}
(18.3656079956, 0.040667515400000555)


----------------------------------------------------------------------------
Training MLP with config/adult.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Namespace(config='config/adult.yml', model_name='MLP', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
On Device: cpu
On Device: cpu
Epoch 0, Val Loss: 0.40640
Epoch 1, Val Loss: 0.37792
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 2, Val Loss: 0.35981
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3575294763274878, 'Log Loss - std': 0.0, 'AUC - mean': 0.8796321219584134, 'AUC - std': 0.0, 'Accuracy - mean': 0.8241977583294948, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8241977583294948, 'F1 score - std': 0.0}
On Device: cpu
Epoch 0, Val Loss: 0.38740
Epoch 1, Val Loss: 0.40186
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 2, Val Loss: 0.36749
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3628432383939052, 'Log Loss - std': 0.005313762066417399, 'AUC - mean': 0.8853003188510449, 'AUC - std': 0.005668196892631372, 'Accuracy - mean': 0.8202684123342806, 'Accuracy - std': 0.003929345995214217, 'F1 score - mean': 0.8202684123342806, 'F1 score - std': 0.003929345995214217}
On Device: cpu
Epoch 0, Val Loss: 0.38475
Epoch 1, Val Loss: 0.36257
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 2, Val Loss: 0.35221
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.35904970497302086, 'Log Loss - std': 0.006899698290244083, 'AUC - mean': 0.8870776204884606, 'AUC - std': 0.00526655227795927, 'Accuracy - mean': 0.8249987613760069, 'Accuracy - std': 0.007419270679331766, 'F1 score - mean': 0.8249987613760069, 'F1 score - std': 0.0074192706793318125}
On Device: cpu
Epoch 0, Val Loss: 0.39991
Epoch 1, Val Loss: 0.36885
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 2, Val Loss: 0.35768
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'Log Loss - mean': 0.3585909958386804, 'Log Loss - std': 0.0060279034151253545, 'AUC - mean': 0.8877662311120855, 'AUC - std': 0.00471433807074396, 'Accuracy - mean': 0.8288995624324966, 'Accuracy - std': 0.009323783014786827, 'F1 score - mean': 0.8288995624324966, 'F1 score - std': 0.009323783014786844}
On Device: cpu
Epoch 0, Val Loss: 0.39923
Epoch 1, Val Loss: 0.37706
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 2, Val Loss: 0.35108
{'Log Loss - mean': 0.3569290974596911, 'Log Loss - std': 0.00633372880332064, 'AUC - mean': 0.8879822403467056, 'AUC - std': 0.0042387057828985174, 'Accuracy - mean': 0.8314857433120906, 'Accuracy - std': 0.009813239522877918, 'F1 score - mean': 0.8314857433120906, 'F1 score - std': 0.009813239522877925}
Results: {'Log Loss - mean': 0.3569290974596911, 'Log Loss - std': 0.00633372880332064, 'AUC - mean': 0.8879822403467056, 'AUC - std': 0.0042387057828985174, 'Accuracy - mean': 0.8314857433120906, 'Accuracy - std': 0.009813239522877918, 'F1 score - mean': 0.8314857433120906, 'F1 score - std': 0.009813239522877925}
Train time: 1.3466067198
Inference time: 0.033047058600000054
Finished cross validation
{'Log Loss - mean': 0.3569290974596911, 'Log Loss - std': 0.00633372880332064, 'AUC - mean': 0.8879822403467056, 'AUC - std': 0.0042387057828985174, 'Accuracy - mean': 0.8314857433120906, 'Accuracy - std': 0.009813239522877918, 'F1 score - mean': 0.8314857433120906, 'F1 score - std': 0.009813239522877925}
(1.3466067198, 0.033047058600000054)


----------------------------------------------------------------------------
Training CatBoost with config/adult.yml in env gbdt

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
TBB Warning: The number of workers is currently limited to 0. The request for 127 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.

Namespace(config='config/adult.yml', model_name='CatBoost', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/tree_models.py", line 112, in fit
    self.model.fit(X, y, eval_set=(X_val, y_val))
  File "/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/catboost/core.py", line 5201, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,
  File "/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/catboost/core.py", line 2396, in _fit
    self._train(
  File "/home/marwan.housni/.conda/envs/gbdt/lib/python3.11/site-packages/catboost/core.py", line 1776, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 4833, in _catboost._CatBoost._train
  File "_catboost.pyx", line 4882, in _catboost._CatBoost._train
_catboost.CatBoostError: /src/catboost/catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: output/CatBoost/Adult/catboost_info


----------------------------------------------------------------------------
Training DecisionTree with config/adult.yml in env sklearn

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/adult.yml', model_name='DecisionTree', dataset='Adult', objective='binary', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='maximize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=14, num_classes=1, cat_idx=[1, 3, 5, 6, 7, 8, 9, 13], cat_dims=[9, 16, 7, 15, 6, 5, 2, 42])
Train model with given hyperparameters
Loading dataset Adult...
Dataset loaded!
(32561, 14)
Scaling the data...
{'Log Loss - mean': 0.40767437306381726, 'Log Loss - std': 0.0, 'AUC - mean': 0.8968444668238383, 'AUC - std': 0.0, 'Accuracy - mean': 0.8460003070781514, 'Accuracy - std': 0.0, 'F1 score - mean': 0.8460003070781514, 'F1 score - std': 0.0}
{'Log Loss - mean': 0.41238783178404004, 'Log Loss - std': 0.004713458720222774, 'AUC - mean': 0.8981630293055194, 'AUC - std': 0.0013185624816810515, 'Accuracy - mean': 0.8523613329002551, 'Accuracy - std': 0.0063610258221036275, 'F1 score - mean': 0.8523613329002551, 'F1 score - std': 0.0063610258221036275}
{'Log Loss - mean': 0.41821099505652976, 'Log Loss - std': 0.00909008194321201, 'AUC - mean': 0.8994146430220865, 'AUC - std': 0.002071749293808508, 'Accuracy - mean': 0.8542257370850185, 'Accuracy - std': 0.005824697897857566, 'F1 score - mean': 0.8542257370850185, 'F1 score - std': 0.005824697897857566}
{'Log Loss - mean': 0.41761063382857305, 'Log Loss - std': 0.007940622967553164, 'AUC - mean': 0.8990978132686867, 'AUC - std': 0.0018762334956543686, 'Accuracy - mean': 0.8538526566221177, 'Accuracy - std': 0.00508555761902022, 'F1 score - mean': 0.8538526566221177, 'F1 score - std': 0.00508555761902022}
{'Log Loss - mean': 0.4202058272429734, 'Log Loss - std': 0.008796755647558562, 'AUC - mean': 0.8993676111467834, 'AUC - std': 0.0017627720454654473, 'Accuracy - mean': 0.8540587837743526, 'Accuracy - std': 0.004567304527689569, 'F1 score - mean': 0.8540587837743526, 'F1 score - std': 0.004567304527689569}
Results: {'Log Loss - mean': 0.4202058272429734, 'Log Loss - std': 0.008796755647558562, 'AUC - mean': 0.8993676111467834, 'AUC - std': 0.0017627720454654473, 'Accuracy - mean': 0.8540587837743526, 'Accuracy - std': 0.004567304527689569, 'F1 score - mean': 0.8540587837743526, 'F1 score - std': 0.004567304527689569}
Train time: 0.08324427780000003
Inference time: 0.0031404218000000484
Finished cross validation
{'Log Loss - mean': 0.4202058272429734, 'Log Loss - std': 0.008796755647558562, 'AUC - mean': 0.8993676111467834, 'AUC - std': 0.0017627720454654473, 'Accuracy - mean': 0.8540587837743526, 'Accuracy - std': 0.004567304527689569, 'F1 score - mean': 0.8540587837743526, 'F1 score - std': 0.004567304527689569}
(0.08324427780000003, 0.0031404218000000484)


----------------------------------------------------------------------------
Training STG with config/covertype.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
Namespace(config='config/covertype.yml', model_name='STG', dataset='Covertype', objective='classification', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='minimize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=54, num_classes=7, cat_idx=None, cat_dims=None)
Train model with given hyperparameters
Loading dataset Covertype...
Dataset loaded!
(581012, 54)
Having 7 classes as target.
Scaling the data...
Traceback (most recent call last):
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 154, in <module>
    main_once(arguments)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 138, in main_once
    sc, time = cross_validation(model, X, y, args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/train.py", line 44, in cross_validation
    loss_history, val_loss_history = curr_model.fit(X_train, y_train, X_test, y_test)  # X_val, y_val)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stochastic_gates.py", line 34, in fit
    loss, val_loss = self.model.fit(X, y, nr_epochs=self.args.epochs, valid_X=X_val, valid_y=y_val,
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 177, in fit
    return self.train(data_loader, nr_epochs, val_data_loader, verbose, meters, early_stop, print_interval)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 234, in train
    self.train_epoch(data_loader, meters=meters)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 215, in train_epoch
    self.train_step(feed_dict, meters=meters)
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/stg.py", line 138, in train_step
    loss = as_float(loss)
           ^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/utils.py", line 266, in as_float
    return stmap(_as_float, obj)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/srv/lustre01/project/manapy-um6p-st-msda-1wabcjwe938/users/marwan.housni/TabSurvey/models/stg_lib/utils.py", line 215, in stmap
    elif isinstance(iterable, (collections.Sequence, collections.UserList)):
                               ^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'collections' has no attribute 'Sequence'


----------------------------------------------------------------------------
Training SAINT with config/covertype.yml in env torch

WARNING:root:imports error 
 You need to install pymongo>=3.9.0 in order to use MongoOutput 
/home/marwan.housni/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Namespace(config='config/covertype.yml', model_name='SAINT', dataset='Covertype', objective='classification', use_gpu=False, gpu_ids=[0, 1], data_parallel=True, optimize_hyperparameters=False, n_trials=2, direction='minimize', num_splits=5, shuffle=True, seed=221, scale=True, target_encode=True, one_hot_encode=False, batch_size=128, val_batch_size=256, early_stopping_rounds=20, epochs=3, logging_period=100, num_features=54, num_classes=7, cat_idx=None, cat_dims=None)
Train model with given hyperparameters
Loading dataset Covertype...
Dataset loaded!
(581012, 54)
Having 7 classes as target.
Scaling the data...
Using dim 8 and batch size 64
Using dim 8 and batch size 64
Epoch 0 loss 0.4283508062362671
Epoch 1 loss 0.34545814990997314
slurmstepd-slurm-a100-gpu-h22a2-u14-sv: error: *** JOB 2319807 ON slurm-a100-gpu-h22a2-u14-sv CANCELLED AT 2024-03-19T00:40:46 DUE TO TIME LIMIT ***
